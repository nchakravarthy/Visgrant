{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "from pandas import DataFrame as df\n",
    "import numpy as np\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "import networkx as nx\n",
    "import plotly.figure_factory as ff\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import folium\n",
    "from folium import plugins\n",
    "from folium.plugins import HeatMap\n",
    "import genderize\n",
    "from genderize import Genderize\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore', category=UserWarning, module='gensim')\n",
    "import gensim\n",
    "import gzip\n",
    "from gensim import corpora, models, similarities\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import TfidfModel\n",
    "from gensim.corpora import Dictionary\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import argparse\n",
    "import lxml\n",
    "from lxml import html\n",
    "import requests\n",
    "import bs4\n",
    "import re\n",
    "import zipfile\n",
    "import io\n",
    "import tensorflow as tf\n",
    "import nltk.data\n",
    "nltk.download('punkt')\n",
    "tokenizer = nltk.data.load('nltk:tokenizers/punkt/english.pickle')\n",
    "import collections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Geographic Map of Organizations Receiving Grants from NIH**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"organization.csv\", names=['organization_name', 'latitude', 'longitude'])\n",
    "m = folium.Map(location=[20,0], tiles='Mapbox Bright', zoom_start=2)\n",
    "for i in range(len(data)):\n",
    "    long = float(data.iloc[i]['longitude'])\n",
    "    lat = float(data.iloc[i]['latitude'])\n",
    "    popup = folium.Popup(data.iloc[i]['organization_name'], parse_html=True) \n",
    "    folium.map.Marker([lat, long], popup).add_to(m)\n",
    "# m.save('organization.html')\n",
    "info = pd.read_csv(\"rganization_information.csv\", names=['organization_name', 'latitude', 'longitude', 'number_pis', 'total_grant_amount'])\n",
    "if info['total_grant_amount'] <= 100000:\n",
    "     \n",
    "#Number of Grants - size\n",
    "#Amount of Grant - color\n",
    "#Number of PI involved per Grant - popup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Heat Map Distribution of Active NIH Grants**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('genderizer.csv', names=['full_name', 'first_name', 'gender', 'latitude', 'longitude'])\n",
    "n = folium.Map([20, 0], tiles= 'Mapbox Bright', zoom_start=2)\n",
    "lat = data['latitude'].astype(float)\n",
    "long = data['longitude'].astype(float)\n",
    "heat_dat = data[['latitude', 'longitude']]\n",
    "heat_dat = heat_dat.dropna(axis=0, subset=['latitude','longitude'])\n",
    "heat_data = [[row['latitude'],row['longitude']] for index, row in heat_dat.iterrows()]\n",
    "HeatMap(heat_data).add_to(n)\n",
    "n.save(\"heatmapnih.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Genderizer - Heat Map Distribution of Female PIs regarding Active NIH Grants**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('genderizerfemale.csv', names=['full_name', 'first_name', 'gender', 'latitude', 'longitude', 'organization'])\n",
    "g = folium.Map([20, 0], tiles= 'Mapbox Bright', zoom_start=2)\n",
    "lat = data['latitude'].astype(float)\n",
    "long = data['longitude'].astype(float)\n",
    "heat_dat = data[['latitude', 'longitude']]\n",
    "heat_dat = heat_dat.dropna(axis=0, subset=['latitude','longitude'])\n",
    "heat_data = [[row['latitude'],row['longitude']] for index, row in heat_dat.iterrows()]\n",
    "HeatMap(heat_data, gradient={.4: 'red', .65: 'green', 1: 'blue'}).add_to(g)\n",
    "g.save(\"gheatmapnih.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gensim**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://projector.tensorflow.org/ with 'abstractnih.csv' uploaded as metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv.field_size_limit(1000000000)\n",
    "dictionary = corpora.Dictionary(line.lower().split() for line in open('abstract.csv'))\n",
    "print(dictionary)\n",
    "class MyCorpus(object):\n",
    "    def __iter__(self):\n",
    "        for line in open('abstract.csv'):\n",
    "            yield dictionary.doc2bow(line.lower().split())\n",
    "corpus_memory_friendly = MyCorpus() \n",
    "print(corpus_memory_friendly)\n",
    "for vector in corpus_memory_friendly:  \n",
    "    print(vector)\n",
    "tfidf = models.TfidfModel(corpus_memory_friendly)\n",
    "corpus_tfidf = tfidf[corpus_memory_friendly]\n",
    "corpus_tfidf.save('word2vecnih.tsv')\n",
    "for doc in corpus_tfidf:\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(text):\n",
    "\n",
    "    text=text.lower()\n",
    "    text = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+)|(pic\\.twitter\\.com/[^\\s]+))','', text)\n",
    "    text = re.sub('@[^\\s]+','', text)\n",
    "    text = re.sub('#([^\\s]+)', '', text)\n",
    "    text = re.sub('[:;>?<=*+()&,\\-#!$%\\{˜|\\}\\[^_\\\\@\\]1234567890’‘]',' ', text)\n",
    "    text = re.sub('[\\d]','', text)\n",
    "    text = text.replace(\".\", '')\n",
    "    text = text.replace(\"'\", '')\n",
    "    text = text.replace(\"`\", '')\n",
    "    text = text.replace(\"'s\", '')\n",
    "    text = text.replace(\"/\", ' ')\n",
    "    text = text.replace(\"\\\"\", ' ')\n",
    "    text = text.replace(\"\\\\\", '')\n",
    "    re.sub(' +', ' ', text)\n",
    "    text=text.replace('\\n', ' ').replace('\\r', ' ').replace('\\t', ' ')\n",
    "    #normalize some utf8 encoding\n",
    "    text = text.replace(\"\\x9d\",'').replace(\"\\x8c\",'')\n",
    "    text = text.replace(\"\\xa0\",'')\n",
    "    text = text.replace(\"\\x9d\\x92\", '').replace(\"\\x9a\\xaa\\xf0\\x9f\\x94\\xb5\", '').replace(\"\\xf0\\x9f\\x91\\x8d\\x87\\xba\\xf0\\x9f\\x87\\xb8\", '').replace(\"\\x9f\",'').replace(\"\\x91\\x8d\",'')\n",
    "    text = text.replace(\"\\xf0\\x9f\\x87\\xba\\xf0\\x9f\\x87\\xb8\",'').replace(\"\\xf0\",'').replace('\\xf0x9f','').replace(\"\\x9f\\x91\\x8d\",'').replace(\"\\x87\\xba\\x87\\xb8\",'')\n",
    "    text = text.replace(\"\\xe2\\x80\\x94\",'').replace(\"\\x9d\\xa4\",'').replace(\"\\x96\\x91\",'').replace(\"\\xe1\\x91\\xac\\xc9\\x8c\\xce\\x90\\xc8\\xbb\\xef\\xbb\\x89\\xd4\\xbc\\xef\\xbb\\x89\\xc5\\xa0\\xc5\\xa0\\xc2\\xb8\",'')\n",
    "    text = text.replace(\"\\xe2\\x80\\x99s\", \"\").replace(\"\\xe2\\x80\\x98\", '').replace(\"\\xe2\\x80\\x99\", '').replace(\"\\xe2\\x80\\x9c\", \"\").replace(\"\\xe2\\x80\\x9d\", \"\")\n",
    "    text = text.replace(\"\\xe2\\x82\\xac\", \"\").replace(\"\\xc2\\xa3\", \"\").replace(\"\\xc2\\xa0\", \"\").replace(\"\\xc2\\xab\", \"\").replace(\"\\xf0\\x9f\\x94\\xb4\", \"\").replace(\"\\xf0\\x9f\\x87\\xba\\xf0\\x9f\\x87\\xb8\\xf0\\x9f\", \"\")\n",
    "    text =  re.sub(r\"\\b[a-z]\\b\", \"\", text)\n",
    "    text=re.sub( '\\s+', ' ', text).strip()\n",
    "    \n",
    "    text=re.sub(r'\\.+', \".\", text)\n",
    "    text=re.sub(r'\\.\\.+', ' ', text).replace('.', '')\n",
    "    # Replace multiple dots with space\n",
    "    text = re.sub('\\.\\.+', ' ', text) \n",
    "    # Remove single dots\n",
    "    text = re.sub('\\.', '', text)\n",
    "    text = re.sub(r'\\.{2,}', ' ', text)\n",
    "    text = re.sub(r'\\.{1}', '', text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('abstract.csv', names=['info'])\n",
    "df = df.astype(str)\n",
    "sentences = df['info'].tolist()\n",
    "normalized_sentences=[]\n",
    "for sentence in sentences:\n",
    "    norm_sent=normalize_text(sentence)\n",
    "    normalized_sentences.append(norm_sent)\n",
    "words = \" \".join(normalized_sentences).split()\n",
    "count = collections.Counter(words).most_common()\n",
    "unique_words = [i[0] for i in count]\n",
    "dic = {w: i for i, w in enumerate(unique_words)}\n",
    "voc_size = len(dic)\n",
    "data = [dic[word] for word in words]\n",
    "cbow_pairs = []\n",
    "for i in range(1, len(data)-1):\n",
    "    cbow_pairs.append([[data[i-1], data[i+1]], data[i]]);\n",
    "cbow_pairs_words = []\n",
    "for i in range(1, len(words)-1):\n",
    "    cbow_pairs_words.append([[words[i-1], words[i+1]], words[i]]);\n",
    "skip_gram_pairs = []\n",
    "for c in cbow_pairs:\n",
    "    skip_gram_pairs.append([c[1], c[0][0]])\n",
    "    skip_gram_pairs.append([c[1], c[0][1]])\n",
    "def get_batch(size):\n",
    "    assert size<len(skip_gram_pairs)\n",
    "    X=[]\n",
    "    Y=[]\n",
    "    rdm = np.random.choice(range(len(skip_gram_pairs)),size, replace=False)\n",
    "    for r in rdm:\n",
    "        X.append(skip_gram_pairs[r][0])\n",
    "        Y.append(skip_gram_pairs[r][1])     \n",
    "    return X , Y\n",
    "batch_size = 20\n",
    "embedding_size = 2\n",
    "num_sampled = 15\n",
    "X = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "Y = tf.placeholder(tf.int32, shape = [batch_size, 1])\n",
    "with tf.device(\"/cpu:0\"):\n",
    "    embeddings = tf.Variable(tf.random_uniform([voc_size, embedding_size], -1.0, 1.0))\n",
    "    embed = tf.nn.embedding_lookup(embeddings, X)\n",
    "nce_weights = tf.Variable(tf.random_uniform([voc_size, embedding_size], -1.0, 1.0))\n",
    "nce_biases = tf.Variable(tf.zeros([voc_size]))\n",
    "loss = tf.reduce_mean(tf.nn.nce_loss(nce_weights, nce_biases, Y, embed, num_sampled, voc_size))\n",
    "optimizer = tf.train.AdamOptimizer(1e-1).minimize(loss)\n",
    "epochs = 10000\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for epoch in range(epochs):\n",
    "        batch_inputs, batch_labels = get_batch(batch_size)\n",
    "        loss_val=sess.run([optimizer,loss], feed_dict = {X : batch_inputs, Y : batch_labels })\n",
    "        if epoch % 1000 == 0:\n",
    "            print(\"Loss at \", epoch, loss_val)\n",
    "trained_embeddings = embeddings.eval()\n",
    "trained_embeddings.save(\"word2vecnih.html\")\n",
    "#if trained_embeddings.shape[1] == 2:\n",
    "   # labels = unique_words[:100]\n",
    "    #for i, label in enumerate(labels):\n",
    "        #x, y = trained_embeddings[i,:]\n",
    "       # plt.scatter(x,y)\n",
    "        #plt.annotate(label, xy=(x, y), xytext=(5, 2), textcoords = 'offset points', ha='right', va='bottom')\n",
    "   # plt.savefig(\"word2vecnih.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Downloading NSF Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('./data'):\n",
    "    os.makedirs('./data')\n",
    "os.chdir('./data')\n",
    "url = \"https://www.nsf.gov/awardsearch/download.jsp\"\n",
    "res = requests.get(url)\n",
    "res.raise_for_status()\n",
    "soup = bs4.BeautifulSoup(res.text, 'html.parser')\n",
    "links = soup.find_all(\"a\", href = re.compile(\"download.*\"))\n",
    "links_correct = []\n",
    "for link in links:\n",
    "    links_correct.append('https://www.nsf.gov/awardsearch/' + link['href'])\n",
    "for i, link in enumerate(links_correct[1:]):\n",
    "    print(\"processing link {} of {}\".format(i, len(links_correct[1:])))\n",
    "    res = requests.get(link)\n",
    "    z = zipfile.ZipFile(io.BytesIO(res.content))\n",
    "    z.extractall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files = [file for file in os.listdir('.') if file.endswith(\".xml\")]\n",
    "\n",
    "with open('../out.csv', 'w') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow( ('file_name', 'directorate', 'division', 'title', 'institution', 'amount', 'grant type', 'abstract', 'date_start', 'date_end', 'program_officer', 'investigators', 'roles', 'number_pis') )\n",
    "    for i, file in enumerate(all_files):\n",
    "        try:\n",
    "            file_name = file\n",
    "            handler = open(file).read()\n",
    "            soup = bs4.BeautifulSoup(handler, 'xml')\n",
    "\n",
    "            directorate = soup.Directorate.LongName.text\n",
    "            division = soup.Division.LongName.text\n",
    "            title = soup.AwardTitle.text\n",
    "\n",
    "            institution = soup.Institution.Name.text\n",
    "\n",
    "            amount = soup.Award.AwardAmount.text\n",
    "            grant_type = soup.Award.AwardInstrument.Value.text\n",
    "            abstract = soup.Award.AbstractNarration.text\n",
    "\n",
    "            date_end = soup.AwardExpirationDate.text\n",
    "            date_start = soup.AwardEffectiveDate.text\n",
    "\n",
    "            program_officer = soup.ProgramOfficer.text\n",
    "\n",
    "            investigators = list()\n",
    "            roles = list()\n",
    "            for investigator in soup.find_all(\"Investigator\"):\n",
    "                investigators.append(investigator.FirstName.text + \" \" + investigator.LastName.text)\n",
    "                roles.append(investigator.RoleCode.text)\n",
    "\n",
    "            number_pis = len(investigators)\n",
    "\n",
    "            try:\n",
    "                writer.writerow( (file_name, directorate, division, title, institution, amount, grant_type, abstract, date_start, date_end, program_officer, investigators, roles, number_pis) )\n",
    "            except:\n",
    "                writer.writerow( ('NA', 'NA', 'NA', 'NA','NA','NA','NA','NA','NA','NA','NA','NA','NA') )\n",
    "                print(\"problem writing the csv row\")\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print(\"on the {}th file\".format(i))\n",
    "csvfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Heat Map Distribution of Continuing NSF Awards**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('nsfheat.csv', names=['file_name', 'directorate', 'division', 'title', 'institution', 'amount', 'grant type', 'abstract', 'date_start', 'date_end', 'investigators', 'roles', 'number_pis', 'latitude', 'longitude'], low_memory=False)\n",
    "f = folium.Map([20, 0], tiles= 'Mapbox Bright', zoom_start=2)\n",
    "lat = data['latitude'].astype(float)\n",
    "long = data['longitude'].astype(float)\n",
    "heat_dat = data[['latitude', 'longitude']]\n",
    "heat_dat = heat_dat.dropna(axis=0, subset=['latitude','longitude'])\n",
    "heat_data = [[row['latitude'],row['longitude']] for index, row in heat_dat.iterrows()]\n",
    "HeatMap(heat_data).add_to(f)\n",
    "f.save(\"heatmapnsf.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Genderizer - Heat Map Distribution of Female PIs regarding Continuing NSF Awards**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('NSFF.csv', names=['latitude', 'longitude', 'name', 'gender'])\n",
    "g = folium.Map([20, 0], tiles= 'Mapbox Bright', zoom_start=2)\n",
    "lat = data['latitude'].astype(float)\n",
    "long = data['longitude'].astype(float)\n",
    "heat_dat = data[['latitude', 'longitude']]\n",
    "heat_dat = heat_dat.dropna(axis=0, subset=['latitude','longitude'])\n",
    "heat_data = [[row['latitude'],row['longitude']] for index, row in heat_dat.iterrows()]\n",
    "HeatMap(heat_data, gradient={.4: 'red', .65: 'green', 1: 'blue'}).add_to(g)\n",
    "g.save(\"gheatmapnsf.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gensim**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://projector.tensorflow.org/ with 'abstractnsf.csv.txt' uploaded as metadata"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
